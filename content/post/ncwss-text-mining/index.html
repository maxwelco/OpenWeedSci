---
title: "North Central Weed Science Society Meeting Proceedings, an Overview of the XXI Century Using Text Analysis"
author: 
- Maxwel C Oliveira
date: 2021-02-14T21:13:14-05:00
categories: ["Text Analysis"]
tags: ["data analysis"]
output: html_document
--- 

<script src="{{< blogdown/postref >}}index_files/header-attrs/header-attrs.js"></script>


<p>This text analysis is part of my abstract (co-authored with <a href="https://twitter.com/WiscWeeds">Rodrigo Werle</a> and Sarah <a href="https://twitter.com/SarahMarinho20">Marinho</a>) presented at the North Central Weed Science Society (<strong>NCWSS</strong>) meeting that was held remotely in December 2020. Here I am presenting part of that abstract, the <strong>weed species</strong> ranked among top 100 words from 2001 through 2020. I am adding to this analysis the 2020 meeting proceedings. Also, I am running the text analysis with much less coding that I have done for the <strong>NCWSS</strong> meeting. That is what I am going to show in this analysis. If you just want to see the figure, please scroll to the bottom of this page.</p>
<p><img src="ncwss.jpg" /></p>
<p>First we have to load the packages needed for this analysis. Please run the codes below:</p>
<pre class="r"><code>library(tidyverse)
library(tidytext)
library(textreadr)
library(pdftools)
library(ggtext)
# if you do not have any of these packages installed, please run install.packages(&quot;name_of_the_package&quot;)</code></pre>
<p>I have downloaded all <strong>NCWSS</strong> proceedings and added into a folder named docs (name of your choice). You can find all PDFs in “code” below the post title (folder docs).</p>
<p>I used the <em>str_c</em> function to get all PDFs, which is into the folder “docs”. The pdfs output contains the path for all 20 <strong>NCWSS</strong> proceedings.</p>
<pre class="r"><code>pdfs &lt;- str_c(&quot;docs&quot;, &quot;/&quot;, list.files(&quot;docs&quot;, pattern = &quot;*.pdf&quot;), 
              sep = &quot;&quot;)
pdfs </code></pre>
<pre><code>##  [1] &quot;docs/nc2001.pdf&quot; &quot;docs/nc2002.pdf&quot; &quot;docs/nc2003.pdf&quot; &quot;docs/nc2004.pdf&quot;
##  [5] &quot;docs/nc2005.pdf&quot; &quot;docs/nc2006.pdf&quot; &quot;docs/nc2007.pdf&quot; &quot;docs/nc2008.pdf&quot;
##  [9] &quot;docs/nc2009.pdf&quot; &quot;docs/nc2010.pdf&quot; &quot;docs/nc2011.pdf&quot; &quot;docs/nc2012.pdf&quot;
## [13] &quot;docs/nc2013.pdf&quot; &quot;docs/nc2014.pdf&quot; &quot;docs/nc2015.pdf&quot; &quot;docs/nc2016.pdf&quot;
## [17] &quot;docs/nc2017.pdf&quot; &quot;docs/nc2018.pdf&quot; &quot;docs/nc2019.pdf&quot; &quot;docs/nc2020.pdf&quot;</code></pre>
<p>Next you will named all these pdfs. If you run the code below, <em>list.files</em> will keep the PDFs names as shown in the code above.</p>
<pre class="r"><code>pdf_names &lt;- list.files(&quot;docs&quot;, pattern = &quot;*.pdf&quot;)
pdf_names</code></pre>
<pre><code>##  [1] &quot;nc2001.pdf&quot; &quot;nc2002.pdf&quot; &quot;nc2003.pdf&quot; &quot;nc2004.pdf&quot; &quot;nc2005.pdf&quot;
##  [6] &quot;nc2006.pdf&quot; &quot;nc2007.pdf&quot; &quot;nc2008.pdf&quot; &quot;nc2009.pdf&quot; &quot;nc2010.pdf&quot;
## [11] &quot;nc2011.pdf&quot; &quot;nc2012.pdf&quot; &quot;nc2013.pdf&quot; &quot;nc2014.pdf&quot; &quot;nc2015.pdf&quot;
## [16] &quot;nc2016.pdf&quot; &quot;nc2017.pdf&quot; &quot;nc2018.pdf&quot; &quot;nc2019.pdf&quot; &quot;nc2020.pdf&quot;</code></pre>
<p>Here is where the magic occurs, I will use the function <em>map</em> of the package <strong>purrr</strong> (tidyverse core). Using <em>map</em> function sames a lot of coding and time.</p>
<p><img src="https://media.giphy.com/media/l2YWs1NexTst9YmFG/giphy.gif" /></p>
<pre class="r"><code>pdfs_text &lt;- map(pdfs, pdftools::pdf_text)</code></pre>
<p>This “magic” is called iteration, so instead of running the analysis by each year we can run it all together. Running <em>pdfs_text</em> alone you get you all proceedings organized as a list. I am not running <em>pdfs_text</em> here because it is a large output. Nonetheless, <em>pdfs_text</em> is not tidy for the analysis yet.</p>
<p>The iteration with <em>map</em> function should be proceeded with a <em>tibble</em> function to organize the proceedings by each year.</p>
<pre class="r"><code>pdf &lt;- tibble(document = pdf_names, text = pdfs_text) %&gt;% 
  mutate(year = 2001:2020) # adding a column for each year
pdf</code></pre>
<pre><code>## # A tibble: 20 x 3
##    document   text         year
##    &lt;chr&gt;      &lt;list&gt;      &lt;int&gt;
##  1 nc2001.pdf &lt;chr [211]&gt;  2001
##  2 nc2002.pdf &lt;chr [211]&gt;  2002
##  3 nc2003.pdf &lt;chr [205]&gt;  2003
##  4 nc2004.pdf &lt;chr [188]&gt;  2004
##  5 nc2005.pdf &lt;chr [229]&gt;  2005
##  6 nc2006.pdf &lt;chr [216]&gt;  2006
##  7 nc2007.pdf &lt;chr [248]&gt;  2007
##  8 nc2008.pdf &lt;chr [215]&gt;  2008
##  9 nc2009.pdf &lt;chr [165]&gt;  2009
## 10 nc2010.pdf &lt;chr [111]&gt;  2010
## 11 nc2011.pdf &lt;chr [174]&gt;  2011
## 12 nc2012.pdf &lt;chr [145]&gt;  2012
## 13 nc2013.pdf &lt;chr [143]&gt;  2013
## 14 nc2014.pdf &lt;chr [107]&gt;  2014
## 15 nc2015.pdf &lt;chr [120]&gt;  2015
## 16 nc2016.pdf &lt;chr [110]&gt;  2016
## 17 nc2017.pdf &lt;chr [136]&gt;  2017
## 18 nc2018.pdf &lt;chr [125]&gt;  2018
## 19 nc2019.pdf &lt;chr [233]&gt;  2019
## 20 nc2020.pdf &lt;chr [174]&gt;  2020</code></pre>
<p>As you can see in the tibble (data frame) above, each proceeding is stored as a list by each year (e.g., &lt;chr [248]&gt;).</p>
<p>Now that we have a tidy tibble, we can proceed with the <a href="https://www.tidytextmining.com/tidytext.html">tokenization</a> using the function <em>unnest_tokens</em>.</p>
<pre class="r"><code>pdf1 &lt;- pdf %&gt;% 
  unnest(text) %&gt;% # pdfs_text is a list
  mutate(text = str_to_lower(text), # making all text lower case
         text = str_replace(text, &quot;2,4-d&quot;, 
                            &quot;twofourd&quot;), # need to replace it
         text = str_replace(text, &quot;marestail&quot;, 
                            &quot;horseweed&quot;)) %&gt;% # marestail = horseweed
  unnest_tokens(word, text, strip_numeric = TRUE)

pdf1 %&gt;% 
  slice_head(n = 10)</code></pre>
<pre><code>## # A tibble: 10 x 3
##    document    year word        
##    &lt;chr&gt;      &lt;int&gt; &lt;chr&gt;       
##  1 nc2001.pdf  2001 industry    
##  2 nc2001.pdf  2001 donations   
##  3 nc2001.pdf  2001 of          
##  4 nc2001.pdf  2001 intellectual
##  5 nc2001.pdf  2001 property    
##  6 nc2001.pdf  2001 rights      
##  7 nc2001.pdf  2001 to          
##  8 nc2001.pdf  2001 universities
##  9 nc2001.pdf  2001 thomas      
## 10 nc2001.pdf  2001 s</code></pre>
<p>You have notice that I used <em>mutate</em> function to change 2,4-D to “twofourd” because tokenization would split it in 2, 4 and D. Also because of their dual names, I have made marestail = horseweed.</p>
<p>Next we need to remove the “stopwords”. Stopwords are words like “in”, “and”, “at”, “their”, “about” etc. The function <em>get_stopwords</em> from tidytext package has five “stopword” sources, I will add them all and stored in <em>stopwords</em>. See below:</p>
<pre class="r"><code>stopwords &lt;- get_stopwords(&quot;en&quot;, source = c(&quot;smart&quot;)) %&gt;% 
  bind_rows(get_stopwords(&quot;en&quot;, source = c(&quot;marimo&quot;))) %&gt;% 
  bind_rows(get_stopwords(&quot;en&quot;, source = c(&quot;nltk&quot;))) %&gt;% 
  bind_rows(get_stopwords(&quot;en&quot;, source = c(&quot;stopwords-iso&quot;))) %&gt;% 
  bind_rows(get_stopwords(&quot;en&quot;, source = c(&quot;snowball&quot;)))

stopwords %&gt;% 
  slice_head(n=10)</code></pre>
<pre><code>## # A tibble: 10 x 2
##    word        lexicon
##    &lt;chr&gt;       &lt;chr&gt;  
##  1 a           smart  
##  2 a&#39;s         smart  
##  3 able        smart  
##  4 about       smart  
##  5 above       smart  
##  6 according   smart  
##  7 accordingly smart  
##  8 across      smart  
##  9 actually    smart  
## 10 after       smart</code></pre>
<p>Now that I have a tibble called “stopwords”, I will use <em>anti_join</em> function to remove the <strong>stopwords</strong> from <strong>pdf1</strong></p>
<pre class="r"><code>pdf2 &lt;- pdf1 %&gt;%
  anti_join(stopwords, by = &quot;word&quot;)</code></pre>
<p>The <em>get_stopwords</em> function with all sources attributes is not enough to remove all words needed for my goal in this analysis. For example, I do not want to have words like “virtual”, “kansas”, “werle”, “proceedings” etc. I have manually made a random “stopwords” for weed science meetings, please check the <a href="https://www.openweedsci.org/post/2020/02/25/2020-wssa/wsws-meeting-program-text-analysis/">WSSA text analysis</a>. I am bringing a “stopword” that I made in my previous analysis in a source code “stop_words.R”. You can find “stop_words.R” in the “code” below the post title.</p>
<pre class="r"><code>source(&quot;stop_words.R&quot;)</code></pre>
<p>I have saved it as <strong>stop_tibble</strong>, which is used also with <em>anti_join</em> function. The <em>anti_join</em> function as described above will remove all “stopwords” in <strong>stop_tibble</strong> from <strong>pdf2</strong>. Notice that here I am also using mutate to bring back 2,4-D.</p>
<pre class="r"><code>pdf3 &lt;- pdf2 %&gt;% 
  anti_join(stop_tibble, by = c(&quot;word&quot;)) %&gt;% # stop_tibble is in the source code
  mutate(word = str_replace(word, &quot;twofourd&quot;, &quot;2,4-d&quot;)) # bring back 2,4-d</code></pre>
<p>Next I will use functions to <em>count</em> words over the years, <em>arrange</em> it as descending, <em>group_by</em> year, rank top 100 words (<em>row_number</em>) and <em>filter</em> the top 100 words by each year.</p>
<pre class="r"><code>pdf4 &lt;- pdf3 %&gt;% 
  count(year, word) %&gt;% 
  arrange(year, -n) %&gt;% 
  group_by(year) %&gt;% 
  mutate(rank = row_number()) %&gt;% 
  filter(rank &lt;= 100)</code></pre>
<p>Now I have the top 100 words by each year:</p>
<pre class="r"><code>pdf4 %&gt;% 
  slice_head(n = 10)</code></pre>
<pre><code>## # A tibble: 200 x 4
## # Groups:   year [20]
##     year word           n  rank
##    &lt;int&gt; &lt;chr&gt;      &lt;int&gt; &lt;int&gt;
##  1  2001 control      716     1
##  2  2001 weed         632     2
##  3  2001 glyphosate   475     3
##  4  2001 applied      427     4
##  5  2001 herbicide    369     5
##  6  2001 corn         358     6
##  7  2001 treatments   339     7
##  8  2001 soybean      257     8
##  9  2001 common       240     9
## 10  2001 yield        226    10
## # … with 190 more rows</code></pre>
<p>In this analysis I am interested only on weeds present in the top 100 words in 2001 and 2020. Therefore, I am using <em>if_else</em> function to create new columns for highlighting selected weed species. You can change and select any word if want as I did it with herbicides in my poster at the 2020 <strong>NCWSS</strong> meeting.</p>
<pre class="r"><code>pdf5 &lt;- pdf4 %&gt;% 
  mutate(highlight = if_else(word %in% c(&quot;amaranth&quot;, &quot;palmer&quot;, 
                                         &quot;kochia&quot;, &quot;horseweed&quot;, 
                                         &quot;grass&quot;, &quot;nightshade&quot;, 
                                         &quot;waterhemp&quot;, &quot;velvetleaf&quot;, 
                                         &quot;ragweed&quot;, &quot;sunflower&quot;,
                                         &quot;foxtail&quot;), TRUE, FALSE),
       variable_col = if_else(highlight == TRUE, word, &quot;NA&quot;))

pdf5 %&gt;% 
  slice_head(n = 5)</code></pre>
<pre><code>## # A tibble: 100 x 6
## # Groups:   year [20]
##     year word           n  rank highlight variable_col
##    &lt;int&gt; &lt;chr&gt;      &lt;int&gt; &lt;int&gt; &lt;lgl&gt;     &lt;chr&gt;       
##  1  2001 control      716     1 FALSE     NA          
##  2  2001 weed         632     2 FALSE     NA          
##  3  2001 glyphosate   475     3 FALSE     NA          
##  4  2001 applied      427     4 FALSE     NA          
##  5  2001 herbicide    369     5 FALSE     NA          
##  6  2002 weed         834     1 FALSE     NA          
##  7  2002 control      658     2 FALSE     NA          
##  8  2002 glyphosate   624     3 FALSE     NA          
##  9  2002 corn         445     4 FALSE     NA          
## 10  2002 applied      330     5 FALSE     NA          
## # … with 90 more rows</code></pre>
<p>Now the tibble is ready. Then, I will proceed with data visualization. First I will set the font family, colors and theme.</p>
<pre class="r"><code>#Set theme
library(extrafont)
extrafont::loadfonts()
font_family &lt;- &#39;Helvetica&#39;
title_family &lt;- &quot;.New York&quot;
background &lt;- &quot;#1D1D1D&quot;
text_colour &lt;- &quot;white&quot;
axis_colour &lt;- &quot;white&quot;
plot_colour &lt;- &quot;black&quot;
theme_style &lt;- theme(text = element_text(family = font_family),
                  rect = element_rect(fill = background),
                  plot.background = element_rect(fill = background, color = NA),
                  plot.title = element_markdown(family = title_family,
                                            face = &#39;bold&#39;, size = 80, colour = text_colour),
                  plot.subtitle = element_markdown(family = title_family, 
                                                   size = 40, colour = text_colour),
                  plot.caption = element_markdown(family = title_family,
                                              size = 25, colour = text_colour, hjust = 0),
                  panel.background = element_rect(fill = background, color = NA),
                  panel.border = element_blank(),
                  panel.grid.major.y = element_blank(),
                  panel.grid.major.x = element_blank(),
                  panel.grid.minor.x = element_blank(),
                  plot.margin = unit(c(3, 0.5, 0.5, 0.5), &quot;cm&quot;), # top, left, bottom, right
                  axis.title.y = element_text(face = &#39;bold&#39;, size = 40, 
                                              colour = text_colour),
                  axis.title.x = element_blank(),
                  axis.text.x.bottom = element_text(size = 45, colour= axis_colour, 
                                                    vjust = 17),
                  axis.text.x.top = element_text(size = 45, colour= axis_colour, 
                                                    vjust = -14),
                  axis.text.y = element_text(size = 30, colour = text_colour),
                  axis.ticks = element_blank(),
                  axis.line = element_blank(),
                  legend.text = element_text(size = 20, colour= text_colour),
                  legend.title = element_text(size = 25, colour= text_colour),
                  legend.position=&quot;none&quot;) 


theme_set(theme_classic() + theme_style)

#Set colour palette
cols &lt;- c(&quot;#F2D9F3&quot;, &quot;#F2D9F3&quot;, &quot;#00E5E5&quot;, &quot;#DEB887&quot;, 
          &quot;#FAC8C8&quot;, &quot;#39393A&quot;, &quot;#FA9664&quot;, 
          &quot;#FF4040&quot;, &quot;#48DE7A&quot;, &quot;#942DC7&quot;, 
          &quot;#F5F5DC&quot;, &quot;#FAFA00&quot;)</code></pre>
<p>Then I will plot the data. The idea here is to see the difference between weeds from top 100 words in 2001 and 2020.</p>
<pre class="r"><code>figure &lt;- pdf5 %&gt;% 
  ggplot(aes(x = year, y = rank, group = word)) +
  geom_line(data = pdf5 %&gt;% filter(variable_col == &quot;NA&quot;),
                                      color = &quot;#39393A&quot;, size = 4) +
  geom_point(data = pdf5 %&gt;% filter(variable_col == &quot;NA&quot;),
                                      color = &quot;#39393A&quot;, size = 10) +
  geom_line(data = pdf5 %&gt;% filter(variable_col != &quot;NA&quot;),
                                       aes(color = variable_col), size = 4) +
  geom_point(data = pdf5 %&gt;% filter(variable_col != &quot;NA&quot;),
                                       aes(color = variable_col), size = 10) +
  scale_y_reverse(breaks = 100:1, sec.axis = dup_axis()) +
  scale_x_continuous(breaks = seq(2001, 2020, 2), limits= c(1999.8, 2021.2), 
                     expand = c(.05, .05), sec.axis = dup_axis()) +
  geom_text(data = pdf5 %&gt;% filter(year == &quot;2001&quot;),
            aes(label = word, x = 2000.8, color = variable_col),
            hjust = &quot;right&quot;,
            fontface = &quot;bold&quot;,
            size = 11) +
  geom_text(data = pdf5 %&gt;%  filter(year == &quot;2020&quot;),
            aes(label = word, x = 2020.2, color = variable_col),
            hjust = &quot;left&quot;,
            fontface = &quot;bold&quot;,
            size = 11) +
  coord_cartesian(ylim = c(101,1)) +
   scale_color_manual(values = cols) +
  labs(title = &quot;&lt;b style=&#39;color:red;&#39;&gt;NCWSS&lt;/b&gt; annual meeting 
       proceedinds text analysis from 2001 through 2020&quot;,
       subtitle = &quot;Figure shows the rank of top 100 words of 2001 (left) 
       and 2020 (right) &lt;b style=&#39;color:red;&#39;&gt;NCWSS&lt;/b&gt; annual meeting proceedings. 
       Common weed species names are highlighed to &lt;br&gt; describe 
       their change across 20 years.&quot;, 
       y= &quot;Rank&quot;,
       caption = &quot;Visualization: @maxwelco adapted from @JaredBraggins | Source: NCWSS&quot;) 


#Export plot 
ggsave(&quot;top_weeds.png&quot;, width = 40, height = 60, dpi=400, limitsize = FALSE, figure)</code></pre>
<p>Check the figure carefully. What were you doing in 2001? What do you think have changed across 20 years? What is the same? Make your own conclusions.</p>
<p><img src="top_weeds.png" /></p>
<p>This figure was adapted from one of <a href="https://twitter.com/JaredBraggins">JaredBraggins</a> Tidy Tuesday visualizations.</p>
<p>This post will be available soon at the <a href="https://www.openweedsci.org/">Open Weed Science</a>.</p>
<hr />
<p>Click <a href="(https://juliasilge.com/blog/learn-tidytext-learnr/)">here</a> to learn more about Tidy Text with Julia Silge.</p>
