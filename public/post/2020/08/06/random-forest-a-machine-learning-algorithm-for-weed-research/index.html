<!DOCTYPE html>
<html lang="en-us">
  <head>
    <meta charset="utf-8">
    <meta name="viewport" content="width=device-width, initial-scale=1">
    
    <title>Random forest: a machine learning algorithm for weed research | Welcome to Open Weed Science</title>
    <link rel="stylesheet" href="/css/style.css" />
    <link rel="stylesheet" href="/css/fonts.css" />
    <link href="//cdn.bootcss.com/highlight.js/9.12.0/styles/github.min.css" rel="stylesheet">

  </head>

  <body class="page">
    <nav class="header">
      <div class="banner">
<a href="/">

<img src="/banner/logo3.png" alt="alternative text on image" />
</a>
</div>

      <div class="head-meta">
      
        <span><a href="/">&larr; Back to Home</a></span>
        <span class="date">2020-08-06</span>
        
        
        
          
        
        
        
        <span><a href="https://github.com/openweedsci/OpenWeedScience/edit/master/content/post/random_forest_2020.Rmd">Edit this page &rarr;</a></span>
        
        
      
      </div>
    </nav>

<div class="container">
<article>
<div class="article-meta">

  <div class="categories">
  
    <a href="/categories/data-analysis">Data Analysis</a>
  
  </div>

  <h1><span class="title">Random forest: a machine learning algorithm for weed research</span></h1>

  
  <h3 class="author">Sarah Striegel
</h3>
  

  
  <p>Tags: <a href="/tags/random-forest">random forest</a>; <a href="/tags/variable-importance">variable importance</a>
  </p>
  
  

</div>



<main>



<div id="background" class="section level2">
<h2>Background</h2>
<p>Random Forest is a machine learning algorithm that generates multiple decision trees using a subsample of bootstrapped observations from randomly selected explanatory variables. Random Forest is a useful tool for variable selection in large and complex datasets for quantitative, discrete, and qualitative variables and has been utilized for response variables such as <em>Amaranthus</em> spp. resistance (<a href="https://doi.org/10.1002/ps.4781">Vieira et al. 2018</a> and <a href="https://doi.org/10.1017/wet.2020.74">Oliveira et al. 2020</a>), weed biomass in cover crops (<a href="https://link.springer.com/article/10.1007/s13593-018-0543-1">Baraibar et al. 2018</a>), soybean yield (<a href="https://doi.org/10.2134/agronj2015.0222">Smidt et al. 2016</a>), soybean injury from dicamba (<a href="https://doi.org/10.1002/ps.5448">Zhang et al. 2019</a>), and Goss’s bacterial wilt and leaf blight development (<a href="https://doi.org/10.1094/PDIS-01-15-0038-RE">Langemeier et al. 2017</a>).</p>
</div>
<div id="getting-started" class="section level2">
<h2>Getting started</h2>
<ul>
<li>Download <strong>R</strong> for your laptop system/desktop from: <a href="https://www.r-project.org" class="uri">https://www.r-project.org</a></li>
</ul>
<p><strong>R</strong> is a programming language and free software environment for statistical computing and graphics supported by the R Foundation for Statistical Computing.</p>
<ul>
<li>Download <strong>RStudio</strong> for your laptop system/desktop from: <a href="https://rstudio.com" class="uri">https://rstudio.com</a></li>
</ul>
<p><strong>RStudio</strong> is an integrated development environment for R, a programming language for statistical computing and graphics.</p>
</div>
<div id="create-a-new-r-project-file" class="section level2">
<h2>Create a new R project file</h2>
<ul>
<li>Click in File -&gt; New project… -&gt; New directory -&gt; New Project -&gt; Save the R work directory anywhere you want in your laptop.</li>
</ul>
<p>The saved file will have an R project and you should copy and paste all your raw data (excel file) in the same work directory (folder).</p>
<ul>
<li>Click in File -&gt; New File -&gt; R script (.R)</li>
</ul>
<p>or</p>
<ul>
<li>Click in File -&gt; New File -&gt; R markdown… (.Rmd)</li>
</ul>
</div>
<div id="install-packages" class="section level2">
<h2>Install packages</h2>
<p>In order to accomplish this exercise, you will need to install the following R packages:</p>
<pre><code>library(parsnip)
library(tidymodels)
library(vctrs)
library(hardhat)
library(tidyverse)
library(ggplot2)
library(RColorBrewer)
library(tidyr)
library(doParallel)
library(ranger)
library(vip)
library(RCurl)</code></pre>
<p>Run all codes above by clicking in the “Run” option in the top right corner of you R script or R markdown. These codes will install the necessary packages. Once you install these packages you will not need to install them again, unless you update R and/or RStudio.</p>
</div>
<div id="data" class="section level2">
<h2>Data</h2>
<p>The data used for this exercise is from a published <a href="https://doi.org/10.1017/wet.2020.89">manuscript</a> by Striegel et al. 2020 investigating the influence of spray additives on spray solution pH. Run the following codes to load the data into R:</p>
<pre class="r"><code>df_path &lt;- getURL(&quot;https://raw.githubusercontent.com/openweedsci/data/master/posts/randomforest2.csv&quot;)

data &lt;- read_csv(df_path) %&gt;% 
  mutate_if(is.character, as.factor)</code></pre>
<p>After running the codes above, the data should appear as <em>data</em> (you could have called it something else).</p>
<pre class="r"><code>data</code></pre>
<pre><code>## # A tibble: 120 x 4
##       ph Rate  AMS   Glyphosate
##    &lt;dbl&gt; &lt;fct&gt; &lt;fct&gt; &lt;fct&gt;     
##  1  6.55 4x    no    no        
##  2  6.54 4x    no    no        
##  3  6.56 4x    no    no        
##  4  6.63 4x    no    no        
##  5  6.62 4x    no    no        
##  6  6.67 4x    no    no        
##  7  4.92 4x    no    yes       
##  8  4.93 4x    no    yes       
##  9  4.93 4x    no    yes       
## 10  4.93 4x    no    yes       
## # … with 110 more rows</code></pre>
<p>The dataset contains solution pH levels for treatments that were a combination of 3 factors: 2 rates (1x and 4x), did/did not include glyphosate, did/did not include a pH buffer.</p>
<p>First, we must split the data into training and testing datasets. We will use the training set to build and tune the model; we will use the testing set at the very end to evaluate the performance of the model. When splitting the dataset, specify the response variable for the strata (“ph”).</p>
<pre class="r"><code>set.seed(123)
s1_split &lt;- initial_split(data, strata = ph)
s1_train &lt;- training(s1_split)
s1_test &lt;- testing(s1_split)</code></pre>
</div>
<div id="building-our-model-and-tuning-hyperparameters" class="section level2">
<h2>Building our Model and Tuning Hyperparameters</h2>
<p>In order to build our Random Forest, we will first build a recipe, then our model, and finally a workflow using both of these.</p>
<p>In the recipe below, we are specifying that “ph” is our response variable, and we are including all other columns in the dataset as explanatory variables. In random forest you can also manipulate your dataset by making <a href="https://en.wikipedia.org/wiki/Dummy_variable_(statistics)">dummy variables</a>.</p>
<p>In tidymodels, there are three hyperparameters for Random Forests: mtry, the number of different predictors sampled at each split; trees, the number of decision trees; and min_n, the minimum number of data points in a node required for further splits. If you are not familiar with these, look at rand_forest in Help tab for clarification on what the parameters represent. Often these parameters are arbitrarily assigned. For example, we are going to set the number of trees to 1000 (trees=1000), because for our purposes it is only important to have “enough.” In the example below, we are specifying that we would like to tune our model for the optimal values of mtry and min_n. </p>
<p>When building your model, there are several options to list for set_mode, depending on your data’s response variable. For this example, our response variable specified is quantitative; thus, we specified the mode as “regression.” “Classification” is a frequently used alternative for qualitative response variables. You can also change the engine specified (in this example we used “ranger”).</p>
<pre class="r"><code>#Build recipe
s1_rec &lt;- recipe(ph ~ ., data=s1_train)

#Build model
tune_spec &lt;- rand_forest(mtry=tune(), trees=1000, min_n=tune()) %&gt;% 
  set_mode(&quot;regression&quot;) %&gt;%
  set_engine(&quot;ranger&quot;) 

#Build your worflow
tune_wf &lt;- workflow() %&gt;%
  add_recipe(s1_rec) %&gt;% 
  add_model(tune_spec) </code></pre>
<p>Now that we have set up our workflow, we are almost ready to tune our hyperparameters. Below we use the vfold_cv function to allow our training dataset to randomly permute the explantory variables. We also use a parallel processor to tune faster. This is perhaps more important for very large datasets. We specified grid=20 when tuning so it would test 20 combinations of mtry and min_n. The larger the grid value, the longer this code will take to run.</p>
<pre class="r"><code>set.seed(234)
s1_folds &lt;- vfold_cv(s1_train)

doParallel::registerDoParallel() 
set.seed(345)
tune_res &lt;- tune_grid(tune_wf, resamples=s1_folds, grid=20) </code></pre>
<pre><code>## i Creating pre-processing data to finalize unknown parameter: mtry</code></pre>
<p>Once the previous chunk has finished running, we can view the results a few different ways.</p>
<p>First, we use the select_best function to select for the optimal values of mtry and min_n given based off our specified criterion, Root Mean Squared Error (RMSE), for the model based on this initial tune. You can change what criterion to select by based on the type of random forest or your personal preference (i.e. I also could have chosen R<sup>2</sup>). RMSE is estimated as the square root of the average difference between the observed and the predicted value squared for all observations (Zhou et al. 2019). Lower RMSE scores indicate better model performance (Zhou et al. 2019). Alternatively, we could have used collect_metrics() to view the results for the entire sampling grid.</p>
<p>Second, we use the next several lines to contstruct a plot to visualize what range of values of mtry and min_n are better for our selection criterion specified. BY looking at this plot, we can decide on a range of values to use for mtry and min_n to better tune the model. Think: is my model better if I have larger or smaller values of mtry?</p>
<pre class="r"><code>tune_res %&gt;%
  select_best(&quot;rmse&quot;) </code></pre>
<pre><code>## # A tibble: 1 x 3
##    mtry min_n .config
##   &lt;int&gt; &lt;int&gt; &lt;chr&gt;  
## 1     3    24 Model15</code></pre>
<pre class="r"><code>tune_res %&gt;%
  collect_metrics() %&gt;% 
  filter(.metric ==&quot;rmse&quot;) %&gt;% 
  pivot_longer(min_n:mtry, values_to=&quot;value&quot;, names_to=&quot;parameter&quot;) %&gt;% 
  ggplot(aes(value, mean, color=parameter)) +
  geom_point(show.legend=FALSE) +
  facet_wrap(~ parameter)</code></pre>
<p><img src="/post/random_forest_2020_files/figure-html/unnamed-chunk-7-1.png" width="672" /></p>
<p>The select_best function tells us that mtry=3 and min_n=5 provide the lowest values of RMSE. Our plot suggests lower values of min_n and mtry are better. We will be using this information to better tune our model by building a new sampling grid based off of what range of values we would like to test for the two parameters. The smaller the range we specify, the more precise we can be with specifying the optimal value for the parameters. Below, we specify we would like to test mtry values from 2-5 and min_n values from 0-5.We also choose to list levels=5. This can be adjusted similarly to as above when we specified grid=20. The larger the value listed for levels, more combinations tested and the longer the code will take to run.</p>
<pre class="r"><code>rf_grid &lt;- grid_regular(mtry(range=c(2,5)), min_n(range=c(0,5)), levels=5)  

set.seed(456)
regular_res &lt;- tune_grid(tune_wf, resamples=s1_folds, grid=rf_grid) 

regular_res %&gt;%
  select_best(&quot;rmse&quot;) </code></pre>
<pre><code>## # A tibble: 1 x 3
##    mtry min_n .config
##   &lt;int&gt; &lt;int&gt; &lt;chr&gt;  
## 1     2     3 Model13</code></pre>
<pre class="r"><code>regular_res %&gt;%
  collect_metrics() %&gt;%
  filter(.metric ==&quot;rmse&quot;) %&gt;% 
  mutate(min_n = factor(min_n)) %&gt;% 
  ggplot(aes(mtry, mean, color = min_n)) +
  geom_line(alpha=0.5, size=1.5) +
  geom_point()</code></pre>
<p><img src="/post/random_forest_2020_files/figure-html/Second%20model%20tune-1.png" width="672" /></p>
<p>Similar to as we had above, we can use select_best function and the plot we construct to determine the best values of mtry and min_n for our model. Select_best tells us that mtry=3 and min_n=1 are optimal.</p>
<p>This time, the plot we construct shows RMSE on the y-axis, values of mtry on the x-axis, and min_n values tested are shown in different colored lines to display the combinations of mtry and min_n parameters and their relationship with RMSE criterion. From this plot, it is clear to see the levels of parameters specified when we ran the select_best function for this tuning do provide the lowest RMSE.</p>
</div>
<div id="finalize-the-model" class="section level2">
<h2>Finalize the Model</h2>
<p>Below we select our model based on our specified criterion, RMSE, and final our model by editing the intial model we had built, tune_spec. Now we have our final model, final_rf.</p>
<p>We chose to utilize the “impurity” measure of importance, but there are several different measures you could list instead. Consult the “ranger” package or the package for the engine selected (if changing) to determine the correct measure to list for your dataset. Variable importance scores provide an estimate of the change in prediction accuracy should the variable be excluded from the model (Wright 2020). Higher VI values indicate the variable is important in the model and in explaining variability of the response variable, while values near zero indicate the variable is not important (Bourgoin et al. 2018; Louppe et al. 2013).</p>
<p>Below we describe three ways to obtain/visualize the variable importance values. First, we can extract the importance values estimated by the model using the vi() function (shown hashtagged). Next, we create a dot plot - this is perhaps the most common display of importance in the literature. Finally, we create a bar plot displaying the same information in the dot plot.</p>
<pre class="r"><code>best_rmse &lt;- select_best(regular_res, &quot;rmse&quot;)

final_rf &lt;- finalize_model(tune_spec, best_rmse)

final_rf %&gt;%
  set_engine(&quot;ranger&quot;, importance=&quot;impurity&quot;) %&gt;% 
  fit(ph ~ ., data=s1_train) %&gt;%
  #vi() %&gt;% 
  #Dot plot
  vip(geom=&quot;point&quot;, horizontal=TRUE, aesthetics=list(color=&quot;black&quot;, size=3)) + 
  theme_light() + 
  theme(plot.title = element_text(hjust=0.5, size=35, face=&quot;bold&quot;),
                     axis.title.x = element_text(size=20, color=&quot;black&quot;), 
                     legend.title = element_blank(),
                     axis.text.x = element_text(size=15, color=&quot;black&quot;),
                     axis.text.y = element_text(size=15, hjust=0, color=&quot;black&quot;),
                     strip.text.x = element_text(size=25, color=&quot;black&quot;, face=&quot;bold&quot;),
                     strip.text = element_text(size=13), 
                     panel.background =element_rect(fill=&quot;white&quot;),
                     panel.grid.major=element_line(color=&quot;white&quot;),
                     panel.grid.minor=element_line(color=&quot;white&quot;)) +
  labs(y=&quot;Variable Importance&quot;) </code></pre>
<p><img src="/post/random_forest_2020_files/figure-html/unnamed-chunk-8-1.png" width="672" /></p>
<pre class="r"><code>final_rf %&gt;%
  set_engine(&quot;ranger&quot;, importance=&quot;impurity&quot;) %&gt;%
  fit(ph ~ ., data=s1_train) %&gt;% 
  #Bar plot
  vip(geom=&quot;col&quot;, horizontal=TRUE, aesthetics=list(fill=c(&quot;#CB181D&quot;, &quot;#EF3B2C&quot;,&quot;#FB6A4A&quot;), 
                                                   width= 0.65)) +
  theme_light() + 
  theme(plot.title = element_text(hjust=0.5, size=35, face=&quot;bold&quot;),
                     axis.title.x = element_text(size=20, color=&quot;black&quot;), 
                     legend.title = element_blank(),
                     axis.text.x = element_text(size=15, color=&quot;black&quot;),
                     axis.text.y = element_text(size=15, hjust=0, color=&quot;black&quot;),
                     strip.text.x = element_text(size=25, color=&quot;black&quot;, face=&quot;bold&quot;),
                     strip.text = element_text(size=13), 
                     panel.background =element_rect(fill=&quot;white&quot;),
                     panel.grid.major=element_line(color=&quot;white&quot;),
                     panel.grid.minor=element_line(color=&quot;white&quot;)) +
  labs(y=&quot;Variable Importance&quot;)</code></pre>
<p><img src="/post/random_forest_2020_files/figure-html/unnamed-chunk-8-2.png" width="672" /></p>
</div>
<div id="test-the-model" class="section level2">
<h2>Test the Model</h2>
<p>Now, let’s say we want to test the model we built. Below we build a final workflow with our final model and fit it to our split dataset - which included both of our training and testing data. When we run collect_metrics, it provides our model selection criterion for the final model when fit to the training data and tested on our testing data.</p>
<p>Finally, when we run collect_predictions, we generate predicted values for our response variable when fit to training data and tested on testing data. This gives us a good idea of how well our model is predicting values based on the explanatory variables included in the model.</p>
<pre class="r"><code>final_wf &lt;- workflow() %&gt;%
  add_recipe(s1_rec) %&gt;% 
  add_model(final_rf) 

final_res &lt;- final_wf %&gt;%
  last_fit(s1_split)

final_res %&gt;%
  collect_metrics() </code></pre>
<pre><code>## # A tibble: 2 x 3
##   .metric .estimator .estimate
##   &lt;chr&gt;   &lt;chr&gt;          &lt;dbl&gt;
## 1 rmse    standard       0.335
## 2 rsq     standard       0.823</code></pre>
<pre class="r"><code>final_res %&gt;%
  collect_predictions() %&gt;% 
  bind_cols(s1_test)</code></pre>
<pre><code>## New names:
## * ph -&gt; ph...4
## * ph -&gt; ph...5</code></pre>
<pre><code>## # A tibble: 28 x 8
##    id               .pred  .row ph...4 ph...5 Rate  AMS   Glyphosate
##    &lt;chr&gt;            &lt;dbl&gt; &lt;int&gt;  &lt;dbl&gt;  &lt;dbl&gt; &lt;fct&gt; &lt;fct&gt; &lt;fct&gt;     
##  1 train/test split  6.51     3   6.56   6.56 4x    no    no        
##  2 train/test split  5.20     9   4.93   4.93 4x    no    yes       
##  3 train/test split  5.20    16   4.96   4.96 4x    no    yes       
##  4 train/test split  6.51    23   6.5    6.5  4x    no    no        
##  5 train/test split  6.51    24   6.53   6.53 4x    no    no        
##  6 train/test split  6.36    25   6.73   6.73 4x    yes   no        
##  7 train/test split  6.36    28   6.8    6.8  4x    yes   no        
##  8 train/test split  6.36    30   6.77   6.77 4x    yes   no        
##  9 train/test split  5.29    32   5.03   5.03 4x    yes   yes       
## 10 train/test split  5.29    37   5.04   5.04 4x    yes   yes       
## # … with 18 more rows</code></pre>
</div>
<div id="references" class="section level2">
<h2>References</h2>
<p>Baraibar B, Mortensen DA, Hunter MC, Barbercheck ME, Kaye JP, Finney DM, Curran WS, Bunchek J, White CM (2018) Growing degree days and cover crop type explain weed biomass in winter cover crops. Agron Sustain Dev 38:1–9</p>
<p>Bourgoin C, Blanc L, Bailly J-S, Cornu G, Berenguer E, Oszwald J, Tritsch I, Laurent F, Hasan AF, Sist P, Gond V (2018) The potential of multisource remote sensing for mapping the biomass of a degraded Amazonian forest. Forests 9:1–21</p>
<p>Langemeier CB, Robertson AE, Wang D, Jackson-Ziems TA, Kruger GR (2017) Factors affecting the development and severity of Goss’s bacterial wilt and leaf blight of corn, caused by <em>Clavibacter michiganensis</em> subsp. <em>nebraskensis</em>. Plant Dis 101:54–61</p>
<p>Louppe G, Wehenkel L, Sutera A, Geurts P (2013) Understanding variable importances in forests of randomized trees. Pages 431–439 in 2013 Proceedings of Neural Information Processing Systems 26. Lake Tahoe, Nevada: Neural Information Processing Systems</p>
<p>Oliveira MC, Giacomini DA, Arsenijevic N, Vieira G, Tranel PJ, Werle R (2020) Distribution and validation of genotypic and phenotypic glyphosate and PPO-inhibitor resistance in Palmer amaranth (<em>Amaranthus palmeri</em>) from southwestern Nebraska. Weed Technol. in press</p>
<p>Smidt ER, Conley SP, Zhu J, Arriaga FJ (2016) Identifying field attributes that predict soybean yield using random forest analysis. Agron J 108:637–646</p>
<p>Vieira BC, Samuelson SL, Alves GS, Gaines TA, Werle R, Kruger GR (2018) Distribution of glyphosate-resistant Amaranthus spp. in Nebraska. Pest Manag Sci 74:2316–2324</p>
<p>Zhang J, Huang Y, Reddy KN, Wang B (2019) Assessing crop damage from dicamba on non-dicamba-tolerant soybean by hyperspectral imaging through machine learning. Pest Manag Sci 75:3260–3272</p>
<p>Zhou J, Li E, Wei H, Li C, Qiao Q, Armaghani DJ (2019) Random Forests and Cubist Algorithms for Predicting Shear Strengths of Rockfill Materials. Appl Sci 9:1–16</p>
</div>

</main>


















<nav class="post-nav">
  <span class="nav-prev"></span>
  <span class="nav-next"><a href="/post/2020/03/18/gwas-data-visualization-in-r/">GWAS data visualization in R &rarr;</a></span>
</nav>


<div id="disqus_thread"></div>
<script>
(function() {
var d = document, s = d.createElement('script');
s.src = 'https://openweedsci.disqus.com/embed.js';
s.setAttribute('data-timestamp', +new Date());
(d.head || d.body).appendChild(s);
})();
</script>
<noscript>Please enable JavaScript to view the <a href="https://disqus.com/?ref_noscript">comments powered by Disqus.</a></noscript>
</article>
</div>

<script async src="//yihui.name/js/center-img.js"></script>

<footer>

<div class="footer">
  <ul class="menu">
    
    <li><a href="/"><span data-hover="Home">Home</span></a></li>
    
    <li><a href="/about/"><span data-hover="About">About</span></a></li>
    
    <li><a href="/contribute/"><span data-hover="Contribute">Contribute</span></a></li>
    
    <li><a href="/categories/"><span data-hover="Categories">Categories</span></a></li>
    
    <li><a href="/tags/"><span data-hover="Tags">Tags</span></a></li>
    
    <li><a href="/index.xml"><span data-hover="Subscribe">Subscribe</span></a></li>
    
  </ul>
  
  <div class="copyright">© <a href="https://openweedsci.org">Open Weed Science</a> 2020 | <a href="https://github.com/openweedsci">Github</a> | <a href="https://twitter.com/openweedsci">Twitter</a></div>
  
</div>
</footer>




<script src="//cdn.bootcss.com/highlight.js/9.12.0/highlight.min.js"></script>
<script src="//cdn.bootcss.com/highlight.js/9.12.0/languages/r.min.js"></script>
<script>
hljs.configure({languages: []});
hljs.initHighlightingOnLoad();
</script>



<script type="application/javascript">
var doNotTrack = false;
if (!doNotTrack) {
	(function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
	(i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
	m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
	})(window,document,'script','https://www.google-analytics.com/analytics.js','ga');
	ga('create', 'UA-154732608-1', 'auto');
	
	ga('send', 'pageview');
}
</script>


</body>
</html>

