<!DOCTYPE html>
<html lang="en-us">
  <head>
    <meta charset="utf-8">
    <meta name="viewport" content="width=device-width, initial-scale=1">
    
    <title>2020 WSSA/WSWS Meeting Program Text Analysis | Welcome to Open Weed Science</title>
    <link rel="stylesheet" href="/css/style.css" />
    <link rel="stylesheet" href="/css/fonts.css" />
    <link href="//cdn.bootcss.com/highlight.js/9.12.0/styles/github.min.css" rel="stylesheet">

  </head>

  <body class="page">
    <nav class="header">
      <div class="banner">
<a href="/">

<img src="/banner/logo3.png" alt="alternative text on image" />
</a>
</div>

      <div class="head-meta">
      
        <span><a href="/">&larr; Back to Home</a></span>
        <span class="date">2020-02-25</span>
        
        
        
          
        
        
        
        <span><a href="https://github.com/openweedsci/OpenWeedScience/edit/master/content/post/2020-02-25-wssa.Rmd">Edit this page &rarr;</a></span>
        
        
      
      </div>
    </nav>

<div class="container">
<article>
<div class="article-meta">

  <div class="categories">
  
    <a href="/categories/text-analysis">Text Analysis</a>
  
  </div>

  <h1><span class="title">2020 WSSA/WSWS Meeting Program Text Analysis</span></h1>

  
  <h3 class="author">Maxwel C Oliveira, Sarah M Matos Marinho, Rodrigo Werle
</h3>
  

  
  <p>Tags: <a href="/tags/text-mining">text mining</a>; <a href="/tags/word-cloud">word cloud</a>
  </p>
  
  

</div>



<main>



<div id="conference" class="section level1">
<h1>Conference</h1>
<p>The 2020 Weed Science Society of America (WSSA) and Western Society of Weed Science (WSWS) joint meeting will be held on Mar 2-5 in Maui (<a href="http://wssa.net/meeting/2020-annual-meeting/" class="uri">http://wssa.net/meeting/2020-annual-meeting/</a>), Hawaii. Unfortunately, some of us will not be present at the meeting but the data analysis discussed herein can help you learn the main Weed Science topics to be discussed in Hawaii.</p>
</div>
<div id="analysis" class="section level1">
<h1>Analysis</h1>
<p>The goal of this text analysis is to evaluate the frequency of words in the 2020 WSSA/WSWS oral and poster titles. The 2020 <a href="http://wssa.net/wp-content/uploads/2020-Program_1_29_Final.pdf">WSSA program is available as a pdf file</a>. In order to achieve our goal with this exercise, you have to download the pdf, load it in R, organize the selected words in a data frame, then in a <a href="https://www.rdocumentation.org/packages/tm/versions/0.7-7/topics/Corpus"><em>corpus</em></a> (collection of text document). Every text analysis requires creating a corpus. In this case, each text document in corpus must be a word. You will see that the process from pdf to <em>corpus</em> requires creating and binding data frames, using <a href="https://www.datamentor.io/r-programming/for-loop/"><em>loop</em></a> functions and other tools specific for text mining in R. Finally, we use <em>ggplot</em> and <em>wordcloud</em> to display the results.</p>
<p>If you want to learn how to analyze the WSSA/WSWS program, we encourage you to read this post in detail.</p>
<p>If you are not interested in learning the analysis itself, scroll down to <strong>Results</strong>.</p>
<div id="install-packages" class="section level2">
<h2>Install packages</h2>
<p>The text analysis of the WSSA/WSWS program will be performed in R statistical software. In order to accomplish this exercise, you will need to install the following R packages:</p>
<pre class="yaml"><code>install.packages(downloader)
install.packages(tidyverse)
install.packages(pdftools)
install.packages(wordcloud)
install.packages(topicmodels)
install.packages(tm)
install.packages(rcorpora)</code></pre>
<p>Run the codes above by clicking in the “Run” option on the top right corner of you R script or R markdown. These codes will install the necessary packages for text analysis. Once you install these packages you will not need to install them again, unless you update R and/or RStudio.</p>
</div>
<div id="import-the-text" class="section level2">
<h2>Import the text</h2>
<p>First you need to find the conference program, which is available in the WSSA website (<a href="http://wssa.net/meeting/2020-annual-meeting/" class="uri">http://wssa.net/meeting/2020-annual-meeting/</a>):</p>
<p><img src="/files/wssa/wssa_cover.png" /></p>
<p>You can either save the pdf program in your local folder together with your R files, or you could use R to download the pdf:</p>
<pre class="yaml"><code>library(downloader)</code></pre>
<p>Copy and past the pdf link to the function <em>download</em>, notice that I named the pdf as ‘wssa.pdf’. The pdf will be saved in your local folder.</p>
<blockquote>
<p>download function</p>
</blockquote>
<pre class="r"><code>wssa &lt;- download(&quot;http://wssa.net/wp-content/uploads/2020-Program_1_29_Final.pdf&quot;, &quot;wssa.pdf&quot;, mode = &quot;wb&quot;)</code></pre>
</div>
<div id="load-the-pdf-file" class="section level2">
<h2>Load the pdf file</h2>
<p>Now, you need to load the pdf in R with <em>pdf_text</em> function. The <em>pdf_text</em> loads the pdf in R and <em>strsplit</em> function breaks the lines in pdf pages.</p>
<pre class="yaml"><code>library(tidyverse)</code></pre>
<blockquote>
<p>pdf_text function</p>
</blockquote>
<blockquote>
<p>strsplit function</p>
</blockquote>
<pre class="r"><code>pdf_full &lt;- pdf_text(&quot;wssa.pdf&quot;) %&gt;% 
  strsplit(split = &quot;\n&quot;)</code></pre>
</div>
<div id="text-analysis" class="section level2">
<h2>Text analysis</h2>
<p>The WSSA/WSWS program (pdf file) contains 155 pages. My goal is to select only the title words of poster and oral presentations.</p>
<div id="select-pages-needed" class="section level3">
<h3>Select pages needed</h3>
<p>We have selected pages 15 to 114 because we are only interested in title words.</p>
<pre class="r"><code>title &lt;- pdf_full[15:114] </code></pre>
<p>For the text analysis, we need to create a single word <a href="http://www.r-tutor.com/r-introduction/vector">vector</a>. The <em>title</em> contains the selected WSSA program pages needed to reach our goal but it is a <a href="http://www.r-tutor.com/r-introduction/list">list</a> of vectors not a single vector.</p>
</div>
<div id="loop" class="section level3">
<h3>Loop</h3>
<p>As described above, nearly 100 pages were selected from the WSSA/WSWS annual meeting program. First, we need to organize the text (<em>title</em>) in a <a href="http://www.r-tutor.com/r-introduction/data-frame">data frame</a>. It would take a long time to convert the list of vectors into a single vector, and then a data frame. Therefore, a <em>loop</em> function is used herein to expedite the process:</p>
<p><img src="https://media3.giphy.com/media/MDXomrcGshGso/200.gif?cid=790b76119d0e67a02063a816058016aadbdb8b9d58790d1d&amp;rid=200.gif" /></p>
<pre class="yaml"><code>titledm &lt;- data.frame() # as data frame

for (i in 1:length(title)) {
  print(i)
  titlelines &lt;- data.frame(names = title[[i]])
  titledm &lt;- bind_rows(titledm, titlelines)
} </code></pre>
<p>The <em>loop</em> function created a data frame with a single variable (<em>names</em>), which is a character vector.</p>
<pre class="r"><code>head(titledm, n=5)</code></pre>
<pre><code>##                                        names
## 1                                    PROGRAM
## 2                   MONDAY AFTERNOON MARCH 2
## 3                            General Session
## 4       LOCATION:              Monarchy #1-4
## 5 TIME:                  04:00 PM - 06:00 PM</code></pre>
<p>As you can see, there are more than a word in each row.</p>
</div>
<div id="tokens-for-titles" class="section level3">
<h3>Tokens for titles</h3>
<p>Tokenization is the act of breaking up a sequence of strings into pieces such as words, keywords, phrases, symbols and other elements called tokens.</p>
<pre class="yaml"><code>library(tidytext)</code></pre>
<p>Befere tokenization, there is onr issue, a common and important name for our analysis is the herbicide 2,4-D. If we do tokenization with 2,4-D, the function will break thw word in 2, 4, -, and D; thus the word will disappear. Changing how 2,4-D is written is a solution to keep it in the analysis.</p>
<pre class="r"><code>titledm$names &lt;- gsub(&quot;2,4-D&quot; , &quot;twofourd&quot; , titledm$names)</code></pre>
<p>Now that 2,4-D is named twofourd, we can proceed with tokenization.</p>
<blockquote>
<p>unnest_tokens function</p>
</blockquote>
<pre class="r"><code>title_tokens &lt;- titledm %&gt;%
  unnest_tokens(words, names)</code></pre>
<p>Let’s run <em>title_tokens</em> to see how the data are organized. Please notice that each row contains a word or number.</p>
<pre class="r"><code>head(title_tokens, n=5)</code></pre>
<pre><code>##         words
## 1     program
## 2      monday
## 2.1 afternoon
## 2.2     march
## 2.3         2</code></pre>
</div>
<div id="removing-authors-name" class="section level3">
<h3>Removing authors’ name</h3>
<p>There are authors’ names and addresses below the titles in the selected pages. It is not our goal to analyze authors’ names; therefore, we have to remove authors’ name from the selected pages. The author index of WSSA/WSWS program is available in pages 116 to 146. Similarly to <em>titles</em>, I will select those pages, do a another <em>loop</em> and create a vector of authors’ names.</p>
<pre class="yalm"><code>authors &lt;- pdf_full[116:146]

authorsdm &lt;- data.frame()

for (i in 1:length(authors)) {
  print(i)
  authorslines &lt;- data.frame(names = authors[[i]])
  authorsdm &lt;- bind_rows(authorsdm, authorslines)
} 

#tokerization
authors_tokens &lt;- authorsdm %&gt;%
  unnest_tokens(words, names)</code></pre>
<p>So far, there are two important vectors, a vector of words from page 15 to 114 (<em>title_tokens</em>) and a vector from page 116 to 146 (<em>authors_tokens</em>). To achieve our goal of removing authors’ name from title pages (<em>title_tokens</em>), a simple solution is to use the <em>anti_join</em> funtion, which removes all words (first and last authors’ names) presented in <em>authors_tokens</em> from <em>title_tokens</em>.</p>
<blockquote>
<p>anti_join funtion</p>
</blockquote>
<pre class="r"><code>titles_tokens_clean &lt;- anti_join(title_tokens, authors_tokens, by = &quot;words&quot;)</code></pre>
</div>
<div id="new-created-vector" class="section level3">
<h3>New created vector</h3>
<p>A new vector is named <em>titles_tokens_clean</em>.</p>
<pre class="r"><code>head(titles_tokens_clean, n = 6)</code></pre>
<pre><code>##       words
## 1   program
## 2 afternoon
## 3     march
## 4   general
## 5   session
## 6  location</code></pre>
<p>A issue that we found is some authors have a number after their last names below the titles. For example Lindquist1, Yearka2, Tenhumberg1, Jhala1, and Werle3, numbers are given to identify their respective institutions.</p>
<p><img src="/files/wssa/werle.png" /></p>
<p>A solution for this problem is to create a data frame with authors’s last names with numbers from 1 to 12. To accomplish this task, we need the <em>loop</em> function again. This is the third <em>loop</em> in this text analysis; by the end of this exercise you will become an expert in using the <em>loop</em> function :)</p>
<pre class="yaml"><code>rm_author &lt;- c(1:12) 
rm_authors_number &lt;-c()
for (i in 1:length(rm_author)) {
  print(i)
  rm_authors_numbers &lt;- c(paste0(authors_tokens$words, i))
  rm_authors_number &lt;- c(rm_authors_number, rm_authors_numbers)
}</code></pre>
<p>Now, let’s create a data frame with column as character.</p>
<pre class="yaml"><code>rm_authors &lt;- data.frame(rm_authors_number) 
rm_authors &lt;- rename(rm_authors, words = rm_authors_number) 
rm_authors$words &lt;- as.character(rm_authors$words)</code></pre>
<pre class="r"><code>rm_authors &lt;- data.frame(rm_authors_number) 
rm_authors &lt;- rename(rm_authors, words = rm_authors_number) 
rm_authors$words &lt;- as.character(rm_authors$words)</code></pre>
<p>Similarly, we run the <em>anti_join</em> funtion again to remove the last names with numbers for our main data frame, now named <em>titles_clean_clean</em>.</p>
<pre class="r"><code>titles_clean_clean &lt;- anti_join(titles_tokens_clean, rm_authors, by = &quot;words&quot;)</code></pre>
<pre class="r"><code>head(titles_clean_clean, n=10)</code></pre>
<pre><code>##        words
## 1    program
## 2  afternoon
## 3      march
## 4    general
## 5    session
## 6   location
## 7   monarchy
## 8       time
## 9         04
## 10        00</code></pre>
</div>
</div>
<div id="stop-words" class="section level2">
<h2>Stop words</h2>
<p>There are some words that we are not interested in, such as march, afternoon, numbers (04, 00), penn etc. Thus, we need to clean the vector <em>titles_tokens_clean</em>.</p>
<p>Stop words are words we may want to remove from the text analysis. For example, we used the package <em>rcorpora</em> to remove words of geography (country, city, capitals, state) and English words such as “a”, “and”, “the”, “of”, “as” etc.</p>
<pre class="yaml"><code>library(rcorpora)</code></pre>
<pre class="r"><code># cities and states
geography &lt;- corpora(&quot;geography/us_cities&quot;) 
cities &lt;- geography$cities
stopwords_cities &lt;- c(cities$city) # stop cities words 
stopwords_cities &lt;- tolower(stopwords_cities)  # making small caps
stopwords_state &lt;- c(cities$state) # stop state words 
stopwords_state &lt;- tolower(stopwords_state) # making lowercase

#country
country &lt;- corpora(&quot;geography/countries&quot;) # stop country words 
stopwords_countries &lt;- c(country$countries) # stop countries words 
stopwords_countries &lt;- tolower(stopwords_countries) # making lowercase

#capitals
capitals &lt;- corpora(&quot;geography/us_state_capitals&quot;)
capitals &lt;- c(capitals$capitals)  # stop US capital words 
stopwords_capitals &lt;- c(capitals$capital) # stop us capitals words 
stopwords_capitals &lt;- tolower(stopwords_capitals) # making lowercase

#english
Enlish &lt;-corpora(&quot;words/stopwords/en&quot;)  
stopwords_en &lt;- c(Enlish$stopWords) # stop english words 
stopwords_en &lt;- tolower(stopwords_en) # making lowercase</code></pre>
<p>The codes above created vectors of capitals, cities, states, country, and English words. Sometimes, the stop words may not be enough, which is the case for the WSSA/WSWS analysis. In this case, we had to manually create a vector of stop words.</p>
<p><img src="https://media3.giphy.com/media/xUVnVIzO42cGu8MHDM/200.gif?cid=790b7611fea7246b64958933d8dab0a9c2cf994bf179a93b&amp;rid=200.gif" /></p>
<pre class="r"><code># words that we want to avoid in the text analysis
stop_wssaprogram_words &lt;- c(&quot;university&quot;, &quot;universidad&quot;, &quot;universidade&quot;, &quot;location&quot;,    
                      &quot;agriscience&quot;, &quot;usda&quot;, &quot;monarchy&quot;, &quot;north&quot;, &quot;march&quot;, &quot;moderator&quot;,
                      &quot;extension&quot;, &quot;college&quot;, &quot;tech&quot;, &quot;time&quot;, &quot;student&quot;, &quot;contest&quot;,
                      &quot;break&quot;, &quot;station&quot;, &quot;basf&quot;, &quot;division&quot;, &quot;ars&quot;, &quot;fort&quot;, &quot;guelph&quot;,
                      &quot;presenter&quot;, &quot;maui&quot;, &quot;monday&quot;, &quot;tuesday&quot;, &quot;wednesday&quot;, &quot;thursday&quot;,
                      &quot;oral&quot;, &quot;poster&quot;, &quot;united&quot;, &quot;western&quot;, &quot;speaker&quot;, &quot;science&quot;, &quot;lleida&quot;,
                      &quot;dakota&quot;, &quot;agriculture&quot;, &quot;service&quot;, &quot;laramie&quot;, &quot;forest&quot;, &quot;piracicaba&quot;,
                      &quot;purdue&quot;, &quot;agrilife&quot;, &quot;center&quot;, &quot;syngenta&quot;, &quot;morning&quot;, &quot;afternoon&quot;,
                      &quot;agri&quot;, &quot;ventenata&quot;, &quot;center&quot;, &quot;east&quot;, &quot;food&quot;, &quot;tifton&quot;, &quot;adelaide&quot;,
                      &quot;pullman&quot;, &quot;resources&quot;, &quot;affiliation&quot;, &quot;saskatoon&quot;, &quot;saskatchewan&quot;,   
                      &quot;moscow&quot;, &quot;park&quot;, &quot;suite&quot;, &quot;west&quot;, &quot;corteva&quot;, &quot;bayer&quot;, &quot;frankfurt&quot;,
                      &quot;cropscience&quot;, &quot;beltsville&quot;, &quot;south&quot;, &quot;northeast&quot;, &quot;northern&quot;,  
                      &quot;northwest&quot;, &quot;platte&quot;, &quot;central&quot;, &quot;cooperative&quot;, &quot;cornell&quot;, &quot;phd&quot;,
                      &quot;program&quot;, &quot;maui&quot;, &quot;clemson&quot;, &quot;stoneville&quot;, &quot;baton&quot;, &quot;rouge&quot;,
                      &quot;ridgetown&quot;, &quot;buenos&quot;, &quot;ithaca&quot;, &quot;são&quot;, &quot;cordoba&quot;, &quot;usa&quot;, &quot;carbondale&quot;,
                      &quot;sur&quot;, &quot;river&quot;, &quot;system&quot;, &quot;johnson&quot;, &quot;athens&quot;, &quot;city&quot;, &quot;usp&quot;, &quot;paulo&quot;,
                      &quot;pelotas&quot;, &quot;aires&quot;, &quot;winfield&quot;, &quot;llc&quot;, &quot;são&quot;, &quot;kingdom&quot;, &quot;pnw&quot;,
                      &quot;lexington&quot;, &quot;eastern&quot;, &quot;mornings&quot;, &quot;brookings&quot;, &quot;southern&quot;, &quot;federal&quot;,
                      &quot;scottsbluff&quot;, &quot;ontario&quot;, &quot;manitoba&quot;, &quot;thorntown&quot;, &quot;lexiton&quot;,
                      &quot;newport&quot;, &quot;macomb&quot;, &quot;penn&quot;, &quot;institute&quot;, &quot;islands&quot;, &quot;summit&quot;,
                      &quot;winnipeg&quot;, &quot;saint&quot;, &quot;fmc&quot;, &quot;brookston&quot;, &quot;southeastern&quot;,
                   &quot;council&quot;, &quot;corporation&quot;, &quot;rio&quot;, &quot;grande&quot;, &quot;harrow&quot;, &quot;aac&quot;, &quot;aafc&quot;,
                   &quot;beach&quot;, &quot;aberdeen&quot;, &quot;brunswick&quot;, &quot;napa&quot;, &quot;rutgers&quot;, &quot;mills&quot;,
                   &quot;valent&quot;,&quot;society&quot;, &quot;starkville&quot;, &quot;department&quot;, &quot;sheridan&quot;, &quot;viçosa&quot;,
                   &quot;lonoke&quot;, &quot;osmond&quot;, &quot;las&quot;, &quot;cruces&quot;, &quot;makawao&quot;, &quot;painter&quot;, &quot;lethbridge&quot;,
                   &quot;daejeon&quot;, &quot;moghu&quot;, &quot;bridgestone&quot;, &quot;princeton&quot;, &quot;lafayette&quot;, &quot;hastings&quot;,
                   &quot;symposium&quot;, &quot;volcano&quot;, &quot;colfax&quot;, &quot;greeley&quot;, &quot;boulder&quot;, &quot;seffner&quot;, &quot;des&quot;,
                   &quot;moines&quot;, &quot;amvac&quot;, &quot;lees&quot;, &quot;crawley&quot;, &quot;kyoto&quot;, &quot;manoa&quot;, &quot;bonham&quot;,
                   &quot;langley&quot;, &quot;waipahu&quot;, &quot;salinas&quot;, &quot;pontotoc&quot;, &quot;richelieu&quot;, &quot;galena&quot;,
                   &quot;beebe&quot;, &quot;calgary&quot;, &quot;bogor&quot;, &quot;jean&quot;, &quot;hampshire&quot;, &quot;trinity&quot;, &quot;triangle&quot;,
                   &quot;york&quot;, &quot;monticello&quot;, &quot;bogotá&quot;, &quot;calgary&quot;, &quot;albion&quot;, &quot;alegre&quot;, &quot;arabia&quot;,
                   &quot;saudi&quot;, &quot;balm&quot;, &quot;bay&quot;, &quot;brooks&quot;, &quot;bury&quot;, &quot;committee&quot;, &quot;csic&quot;, &quot;daniel&quot;,
                   &quot;dow&quot;, &quot;edmonton&quot;, &quot;edmunds&quot;, &quot;edwardsville&quot;, &quot;eloy&quot;, &quot;esalq&quot;,&quot;goldsboro&quot;,
                   &quot;hettinger&quot;, &quot;iaa&quot;, &quot;korea&quot;, &quot;lacombe&quot;, &quot;jaboticabal&quot;, &quot;keenesburg&quot;,
                   &quot;marina&quot;, &quot;mcminnville&quot;, &quot;maringa&quot;, &quot;marrone&quot;, &quot;morrisville&quot;, &quot;monterey&quot;,
                   &quot;nufarm&quot;, &quot;oak&quot;, &quot;ncsu&quot;, &quot;nacional&quot;, &quot;ottawa&quot;, &quot;pendleton&quot;, &quot;pat&quot;,
                   &quot;porto&quot;, &quot;plc&quot;, &quot;stephenville&quot;, &quot;steckel&quot;, &quot;tokyo&quot;, &quot;universitat&quot;, &quot;vero&quot;,
                   &quot;univ&quot;, &quot;wageningen&quot;, &quot;wooster&quot;, &quot;christi&quot;, &quot;corpus&quot;, &quot;company&quot;, &quot;dryden&quot;,
                   &quot;glenn&quot;, &quot;nice&quot;, &quot;nrcs&quot;, &quot;oroville&quot;, &quot;susanville&quot;, &quot;amworth&quot;, &quot;yuba&quot;,
                   &quot;hays&quot;, &quot;bracknell&quot;, &quot;agcenter&quot;, &quot;americas&quot;, &quot;america&quot;, &quot;immokalee&quot;,
                   &quot;harpenden&quot;, &quot;rothamsted&quot;, &quot;wagga&quot;, &quot;county&quot;, &quot;hawaiian&quot;,
                   &quot;national&quot;, &quot;pacific&quot;, &quot;adams&quot;)</code></pre>
<p>Now, we will combine all stop words into a single vector:</p>
<pre class="r"><code>stopwords_titles &lt;- c(stopwords_cities, stopwords_state, stopwords_countries, stopwords_capitals,
                      stopwords_en,  stop_wssaprogram_words)</code></pre>
</div>
<div id="corpus" class="section level2">
<h2>Corpus</h2>
<p>Text analysis methodology requires to convert any text being analyzed into a <em>corpus</em>.</p>
<pre class="yaml"><code>library(topicmodels)
library(tm)</code></pre>
<pre class="r"><code>title_source &lt;- VectorSource(titles_clean_clean$words)
title_corpus &lt;- VCorpus(title_source)</code></pre>
<div id="cleaning-the-text" class="section level3">
<h3>Cleaning the text</h3>
<p>Now, it is time to use all stop words (<em>stopwords_titles</em>) to clean the data frame.</p>
<pre class="r"><code># this code section will take long (~3 min) to run. 
clean_corpus &lt;- function(corpus){
  corpus &lt;- tm_map(corpus, removePunctuation) # remove punctuation
  corpus &lt;- tm_map(corpus, content_transformer(tolower)) # remove punctuation
  corpus &lt;- tm_map(corpus, removeNumbers) # remove numbers
  corpus &lt;- tm_map(corpus, removeWords, stopwords_titles) # remove stop words
  corpus
}

title_corpus &lt;- clean_corpus(title_corpus)</code></pre>
</div>
<div id="creating-a-matrix" class="section level3">
<h3>Creating a matrix</h3>
<p>To plot the text analysis using <em>wordcloud</em> or <em>ggplot</em> function in R, we have to convert <em>VCorpus</em> format into a <em>TermDocumentMatrix</em>, then a <em>data.frame</em>.</p>
<pre class="r"><code>title_dtm &lt;- TermDocumentMatrix(title_corpus)
title_matrix &lt;- as.matrix(title_dtm)
title_v &lt;- sort(rowSums(title_matrix), decreasing=TRUE)
title_dt &lt;- data.frame(word = names(title_v),freq=title_v)</code></pre>
<p>Remember when we changed the 2,4-D name format? Now it is time to bring back the name 2,4-D back to the analysis.</p>
<pre class="r"><code>title_dt$word &lt;- gsub(&quot;twofourd&quot; , &quot;2,4-D&quot; , title_dt$word)</code></pre>
<p>We are now done with text analysis!</p>
</div>
</div>
</div>
<div id="results" class="section level1">
<h1>Results</h1>
<div id="bar-chart" class="section level2">
<h2>Bar chart</h2>
<p>The most frequent word in the the 2020 WSSA/WSWS meeting is <strong>weed</strong> (n=137). Also, the plural <strong>weeds</strong> (n=38) is shown in the WSSA/WSWS program (Figure 1).</p>
<p><img src="https://media2.giphy.com/media/Qw7gtQtQUa5AFk7WQQ/giphy.gif?cid=790b7611fa855b80e72dc98fe62c33df73bd0e5aaef4bde7&amp;rid=giphy.gif" /></p>
<p><strong>No surprises!</strong></p>
<pre class="r"><code>title_dt %&gt;% 
  filter(freq &gt;= 30) %&gt;% 
  ggplot(aes(x=reorder(word,freq), y=freq, fill=freq)) +
  scale_fill_continuous(low = &quot;#FFF6F6&quot;, high = &quot;#9b0000&quot;) +
  geom_col(show.legend=FALSE) + coord_flip() + theme_classic() +
  labs(y=&quot;Frequence (n)&quot;, x=&quot;Words&quot;) + ylim(0,150) +
  theme(axis.title = element_text(size=15),
        axis.text = element_text(size=13)) </code></pre>
<div class="figure"><span id="fig:unnamed-chunk-24"></span>
<img src="/post/2020-02-25-wssa_files/figure-html/unnamed-chunk-24-1.png" alt="Most frequent words (n=30+) that appeared in the WSSA/WSWS oral and poster presentation titles." width="672" />
<p class="caption">
Figure 1: Most frequent words (n=30+) that appeared in the WSSA/WSWS oral and poster presentation titles.
</p>
</div>
<p>Other words appear 30+ times in this WSSA/WSWS program:</p>
<ul>
<li><p>Interesting to see how <strong>herbicide</strong> (n=119) and <strong>herbicides</strong> (n=59) used for weed <strong>control</strong> (n=104) dominate the program.</p></li>
<li><p><strong>Efficacy</strong> (n=34) is also a commonly used word likely related to chemical weed <strong>management</strong> (n=77).</p></li>
<li><p><strong>Dicamba</strong> (n=59) appeared more than <strong>glyphosate</strong> (n=45), likely due to recent introduction of dicamba resistant crops, benefits and some of the challenges associated with the herbicide.</p></li>
<li><p>In 2020, <strong>resistance</strong> (n=61), <strong>resistant</strong> (n=43), and <strong>amaranthus</strong> (n=43) are still major topics in the WSSA/WSWS meeting.</p></li>
<li><p><strong>Seed</strong> (n=35) is likely due to increase research on Harrington Seed Destructor strategy for weed management.</p></li>
<li><p><strong>Soybean</strong> (n=33) and <strong>cotton</strong> (n=32) are the most frequent crops mentioned in the WSSA/WSWS program.</p></li>
</ul>
</div>
<div id="word-cloud" class="section level2">
<h2>Word cloud</h2>
<p>The word cloud provides a big picture of the WSSA/WSWS meeting program. It shows words that appeared at least twice in the program. Bigger words indicate higher frequency in the program and vice versa. Same colors indicate word frequency of the same magnitude.</p>
<pre class="yaml"><code>library(wordcloud)</code></pre>
<blockquote>
<p>wordcloud function</p>
</blockquote>
<pre class="r"><code>pal &lt;- brewer.pal(8,&quot;Dark2&quot;) #selecting the color palette

set.seed(1234)
wordcloud(words = title_dt$word, freq = title_dt$freq, min.freq=2, scale=c(5,.1),
           random.order=FALSE, rot.per=0.35, colors=pal)</code></pre>
<p><img src="/post/2020-02-25-wssa_files/figure-html/unnamed-chunk-25-1.png" width="672" /></p>
<p>The word cloud highlights how herbicides dominate the weed science discipline.</p>
<p><strong>Challenge:</strong></p>
<ol style="list-style-type: decimal">
<li><p>What are the dominant weeds in the word cloud?</p></li>
<li><p>What are the dominat herbicides in the word cloud?</p></li>
<li><p>Can you find the words: ecosystem, ecology, sustainable and agroecosystem?</p></li>
</ol>
<p>Spend some time cruising through the big and small words. What are your thoughts? Which words do you think are missing in the program? Should some words be bigger than others?</p>
</div>
</div>
<div id="acknowledgements" class="section level1">
<h1>Acknowledgements</h1>
<p>We would like to acknowledge Nick Arneson for reviewing this post</p>
<p><img src="https://media0.giphy.com/media/yrSRNzatoNPsk/200.gif?cid=790b761127566fe545bb5c46d7dff064e7d876b321942adf&amp;rid=200.gif" /></p>
</div>

</main>























<section class="article-meta article-footer">
  <h3>About the Author</h3>
  
    <p>Maxwel C Oliveira is currently a Professor of Weed Science/Data Analysis at <a href="https://www.unoeste.br">Western Sao Paulo University</a> and Researcher at <a href="https://www.wisc.edu">University of Wisconsin-Madison</a>. You can follow him on <a href="https://twitter.com/maxwelco">Twitter</a></p>
  
    <p>Sarah M Matos Marinho is currently a Research Assistant at <a href="http://www.cepesp.io/en/">Fundação Getulio Vargas-CEPESP São Paulo</a>. You can find more information about her at <a href="http://lattes.cnpq.br/9817226069944290">http://lattes.cnpq.br/9817226069944290</a>.</p>
  
    <p>Rodrigo Werle is currently an Assistant Professor of Weed Science at the University of Wisconsin-Madison. You can find more information about him at <a href="https://wiscweeds.info">https://wiscweeds.info</a>.</p>
  
</section>






<nav class="post-nav">
  <span class="nav-prev"><a href="/post/2020/03/18/gwas-data-visualization-in-r/">&larr; GWAS data visualization in R</a></span>
  <span class="nav-next"><a href="/post/2020/02/10/simulating-power/">Simulating Power &rarr;</a></span>
</nav>


<div id="disqus_thread"></div>
<script>
(function() {
var d = document, s = d.createElement('script');
s.src = 'https://openweedsci.disqus.com/embed.js';
s.setAttribute('data-timestamp', +new Date());
(d.head || d.body).appendChild(s);
})();
</script>
<noscript>Please enable JavaScript to view the <a href="https://disqus.com/?ref_noscript">comments powered by Disqus.</a></noscript>
</article>
</div>

<script async src="//yihui.name/js/center-img.js"></script>

<footer>

<div class="footer">
  <ul class="menu">
    
    <li><a href="/"><span data-hover="Home">Home</span></a></li>
    
    <li><a href="/about/"><span data-hover="About">About</span></a></li>
    
    <li><a href="/contribute/"><span data-hover="Contribute">Contribute</span></a></li>
    
    <li><a href="/categories/"><span data-hover="Categories">Categories</span></a></li>
    
    <li><a href="/tags/"><span data-hover="Tags">Tags</span></a></li>
    
    <li><a href="/index.xml"><span data-hover="Subscribe">Subscribe</span></a></li>
    
  </ul>
  
  <div class="copyright">© <a href="https://openweedsci.org">Open Weed Science</a> 2020 | <a href="https://github.com/openweedsci">Github</a> | <a href="https://twitter.com/openweedsci">Twitter</a></div>
  
</div>
</footer>


<script src="//yihui.name/js/math-code.js"></script>
<script async src="//mathjax.rstudio.com/latest/MathJax.js?config=TeX-MML-AM_CHTML"></script>



<script src="//cdn.bootcss.com/highlight.js/9.12.0/highlight.min.js"></script>
<script src="//cdn.bootcss.com/highlight.js/9.12.0/languages/r.min.js"></script>
<script>
hljs.configure({languages: []});
hljs.initHighlightingOnLoad();
</script>



<script type="application/javascript">
var doNotTrack = false;
if (!doNotTrack) {
	(function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
	(i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
	m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
	})(window,document,'script','https://www.google-analytics.com/analytics.js','ga');
	ga('create', 'UA-154732608-1', 'auto');
	
	ga('send', 'pageview');
}
</script>


</body>
</html>

